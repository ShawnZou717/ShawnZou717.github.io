<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://shawnzou717.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shawnzou717.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-31T12:04:33+08:00</updated><id>https://shawnzou717.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Transformer Analysis</title><link href="https://shawnzou717.github.io/blog/2023/transformer-analysis/" rel="alternate" type="text/html" title="Transformer Analysis"/><published>2023-08-03T11:57:00+08:00</published><updated>2023-08-03T11:57:00+08:00</updated><id>https://shawnzou717.github.io/blog/2023/transformer-analysis</id><content type="html" xml:base="https://shawnzou717.github.io/blog/2023/transformer-analysis/"><![CDATA[<h2 id="table-of-contents">Table of Contents</h2> <ul> <li><a href="#table-of-contents">Table of Contents</a></li> <li><a href="#transformer-architecture">Transformer Architecture</a></li> <li><a href="#forward-propagation">Forward Propagation</a> <ul> <li><a href="#word-embedding">Word Embedding</a></li> </ul> </li> </ul> <p>Recently, a great personal assistant and chit-chat robot ChatGPT gained a lot of attention. Unlike other wooden voice assistants on our phones such as Siri, ChatGPT can process textual and semantic information in natural language, meaning it can talk to people continuously on a given topic and understand undelying meanings. Find it hard to believe? Let’s have a try.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/ChatGPT_joke-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/ChatGPT_joke-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/ChatGPT_joke-1400.webp"/> <img src="/assets/img/transformer-analysis/ChatGPT_joke.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>You can see that not only can ChatGPT keep the conversation but also give natural responses to my PhD joke. Why is it so fluent and elegant? What sets ChatGPT apart from Siri and Bixby? The reason is the powerful informatic extraction ability of ChatGPT’s submodule, Transformer. Transformer’s application extends way out of NLP domain. As a powerful mathematical tool, it has helped us in DNA recognition, medical research and many aspects in other research area. I believe it is safe to say that one day we may all need to apply this model in our project. Thus, a solid understanding of Transformer architecture is neccesary. To this end, this blog focuses on a comprehensive introduction of Transformer.</p> <h2 id="transformer-architecture">Transformer Architecture</h2> <hr/> <p>In 2017, Google posted a paper named <a href="https://arxiv.org/abs/1706.03762v4">Attention is All You Need</a> in arXiv bringing Transformer into history. Though Transformer follows the <code class="language-plaintext highlighter-rouge">seq2seq</code> structure (also known as <code class="language-plaintext highlighter-rouge">decoders and encoders</code>), its encoders and decoders consist of sole <code class="language-plaintext highlighter-rouge">self-attention</code> modules instead of <code class="language-plaintext highlighter-rouge">RNN</code> and <code class="language-plaintext highlighter-rouge">CNN</code> like most other NLP models. This is exactly the origin of the article title, a neural network composed entirely of self-attention mechanisms. Now let’s take the classic Transformer as an example reviewing this unique model. Below shows a simplified Transformer structure and detail composition of encoders and decoders.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/transformer-entire-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/transformer-entire-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/transformer-entire-1400.webp"/> <img src="/assets/img/transformer-analysis/transformer-entire.png" class="img-fluid d-block mx-auto rounded z-depth-1" width="auto" height="60%" max-width="60%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Simplified Transformer Structure </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/transformer-en-decoders-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/transformer-en-decoders-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/transformer-en-decoders-1400.webp"/> <img src="/assets/img/transformer-analysis/transformer-en-decoders.png" class="img-fluid d-block mx-auto rounded z-depth-1" width="auto" height="60%" max-width="60%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Transformer encoders and decoders </div> <p>Transformer model stacks 6 encoders and 6 decoders, ahead by a source embedding and a target embedding networks, end with a softmax classifier. Embedding units can be self-constructed algorithms or pre-trained networks such as <code class="language-plaintext highlighter-rouge">Word2Vec</code>. The encoder and decoder have a very similar structure, only that the decoder has one extra layer of <code class="language-plaintext highlighter-rouge">ADD &amp; NORM</code> and one extra layer of <code class="language-plaintext highlighter-rouge">multi-head attention</code>. In addition, it should be noted that a <code class="language-plaintext highlighter-rouge">mask operation</code> has been added to the 1st multi-head attention layer of each decoder, which aims to use the seq2seq structure while ensuring the parallelism of the network. The softmax classifier consists of a <code class="language-plaintext highlighter-rouge">linear transformation</code> and a <code class="language-plaintext highlighter-rouge">softmax</code> layer. To know more technique details of these operators, let’s start orgnize the forward propagation of Transformer.</p> <h2 id="forward-propagation">Forward Propagation</h2> <hr/> <h3 id="word-embedding">Word Embedding</h3> <p>Before we go further, I’d like to give you some preliminary knowledge about word encoding, that is the method for digitalizing human words as computers are not capable of recognizing human language sympols. The most widely used ones are <code class="language-plaintext highlighter-rouge">one-hot encoding</code> and <code class="language-plaintext highlighter-rouge">word embedding</code> methods.</p> <ul> <li> <p><strong>One-hot encoding</strong></p> <p>One-hot encoding is a technique that transfers words into binary vector where each value represent one word in the lexicon. Fot a given word, the value at its corresponding position is turned on (set to be 1) while the others remain turned off (set to be 0). For example, we have RGB Tricolor, each color can be expressed by the following vectors:</p> <table> <thead> <tr> <th style="text-align: center">Color</th> <th style="text-align: center">Vector</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">red</td> <td style="text-align: center">[1, 0, 0]</td> </tr> <tr> <td style="text-align: center">green</td> <td style="text-align: center">[0, 1, 0]</td> </tr> <tr> <td style="text-align: center">blue</td> <td style="text-align: center">[0, 0, 1]</td> </tr> </tbody> </table> <p>Though one-hot encoding solves the words digitalizing problem, the produced vector cannot be directly applied in the neural networks. Because neural networks are actually nothing else but layered matrix computations. While one-hot vectors combined together form a sparse matrix. This kind of matrix has far more 0 valued elements than other effective elements leading to a very low informatic density, i.e. a very small informatic volume and an extremely large matrix size. As a result, neural network models would suffer from huge computation complexity increases and only gain very poor language processing ability.</p> </li> <li> <p><strong>Word embedding</strong></p> <p>To tackle this problem, <code class="language-plaintext highlighter-rouge">word embedding</code> is proposed. This method aims to represent words with a intensive vector in a high dimension space, meanwhile the distance among vectors represent the textual and semantic similarity of their corresponding words. For example, all the words descripe emotions such as <strong>happy</strong> and <strong>sad</strong> should concentrate in one area after word embedding.</p> </li> </ul> <p>There are some widely used word embedding methods including <code class="language-plaintext highlighter-rouge">Word2Vec</code>, <code class="language-plaintext highlighter-rouge">FastText</code> and <code class="language-plaintext highlighter-rouge">GloVe</code>. Wanna know more? Try asking ChatGPT about it. Classic Transformer uses a customized embedding unit containing a linear Transformation and a softmax layer. To specify the parameter shape of classic Transformer, we assume this Transformer is used for translating English to Germany with both 32,000 words lexicon.</p> <p>The input of word embedding network is <code class="language-plaintext highlighter-rouge">ont-hot</code> word vector. Say we have 32000 words in a English lexicon ranking in dictionary order. <code class="language-plaintext highlighter-rouge">One hot</code> method transforms each word to a vector of length 32000 with 31999 zero values and 1 one value locating at the word’s index in lexicon. For example, we all know (At least every Chinese student knows) the <strong>abandon</strong> ranks no.1 in all High School English dictionaries of China. That make <strong>abandon</strong>’s <code class="language-plaintext highlighter-rouge">one-hot</code> vector be $[1, 0, 0, …0]$, vector length = 32000. Same as other words.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/one-hot-encoding-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/one-hot-encoding-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/one-hot-encoding-1400.webp"/> <img src="/assets/img/transformer-analysis/one-hot-encoding.jpg" class="img-fluid d-block mx-auto rounded z-depth-1" width="auto" height="60%" max-width="60%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> One hot encoding </div> <p><code class="language-plaintext highlighter-rouge">Word embedding</code> maps words into intensive vectors. An intensive vector is such a vector of length far smaller than the dictionary size and indicating the relationship of words by their space distrution. The needs of <code class="language-plaintext highlighter-rouge">word embedding</code> encoding come from the intensive relevance of human word. Though <code class="language-plaintext highlighter-rouge">one hot</code> encoding gives accurate representation of each word, these vector are orthogonal to each other, that makes</p> <p>For example, in <a href="https://arxiv.org/abs/1706.03762v4">Attention is All You Need</a>, the intensive word vector size is set to be 512, while the <code class="language-plaintext highlighter-rouge">one-hot</code> vector size is 32000. Compressing the vector size can not only avoid the delimma of</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/word-embedding-space-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/word-embedding-space-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/word-embedding-space-1400.webp"/> <img src="/assets/img/transformer-analysis/word-embedding-space.jpeg" class="img-fluid d-block mx-auto rounded z-depth-1" width="auto" height="60%" max-width="60%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Word embedding space (Chinese characters as an example), come from csdn blog (https://blog.csdn.net/Alex_81D/article/details/114287498) </div>]]></content><author><name></name></author><category term="deep-learning"/><category term="Transformer"/><summary type="html"><![CDATA[a plain introduction of Transformer and professional derivation of its forward and backward propagation]]></summary></entry></feed>