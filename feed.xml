<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://shawnzou717.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shawnzou717.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-19T19:25:52+08:00</updated><id>https://shawnzou717.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Transformer Analysis</title><link href="https://shawnzou717.github.io/blog/2023/transformer-analysis/" rel="alternate" type="text/html" title="Transformer Analysis"/><published>2023-08-03T11:57:00+08:00</published><updated>2023-08-03T11:57:00+08:00</updated><id>https://shawnzou717.github.io/blog/2023/transformer-analysis</id><content type="html" xml:base="https://shawnzou717.github.io/blog/2023/transformer-analysis/"><![CDATA[<h2 id="table-of-contents">Table of Contents</h2> <ul> <li><a href="#table-of-contents">Table of Contents</a></li> <li><a href="#transformer-architecture">Transformer Architecture</a></li> <li><a href="#forward-propagation">Forward Propagation</a> <ul> <li><a href="#word-embedding">Word Embedding</a></li> </ul> </li> </ul> <p>Among all the deep learning models, Transformer can be the most widely talked over one thanks to the tremendous performance that ChatGPT, a Transformer-based model, has shown in being a personal assistant and chatbot. Never heard of it? Let’s take a look at ChatGPT’s own words.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/try-chatGPT-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/try-chatGPT-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/try-chatGPT-1400.webp"/> <img src="/assets/img/transformer-analysis/try-chatGPT.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> How ChatGPT introduces Transformer model </div> <p>Transformer’s application extends way out of NLP domain. As a powerful mathematical tool, it has helped us in DNA recognition, medical research and many aspects in other research area. I believe it is safe to say that one day we may all need to apply this model in our project. Thus, a solid understanding of Transformer architecture is neccesary. To this end, this blog focuses on a comprehensive introduction of Transformer.</p> <h2 id="transformer-architecture">Transformer Architecture</h2> <hr/> <p>In 2017, Google posted a paper named <a href="https://arxiv.org/abs/1706.03762v4">Attention is All You Need</a> in arXiv bringing Transformer into history. Though Transformer follows the <code class="language-plaintext highlighter-rouge">seq2seq</code> structure (also known as <code class="language-plaintext highlighter-rouge">decoders and encoders</code>), its encoders and decoders consist of sole <code class="language-plaintext highlighter-rouge">self-attention</code> modules instead of <code class="language-plaintext highlighter-rouge">RNN</code> and <code class="language-plaintext highlighter-rouge">CNN</code> like most other NLP models. This is exactly the origin of the article title, a neural network composed entirely of <code class="language-plaintext highlighter-rouge">self-attention</code> mechanisms. Now let’s take the classic Transformer as an example reviewing this unique model. Below shows the simplified structure of Transformer and detailed composition of encoders and decoders.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/transformer-entire-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/transformer-entire-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/transformer-entire-1400.webp"/> <img src="/assets/img/transformer-analysis/transformer-entire.png" class="img-fluid d-block mx-auto rounded z-depth-1" width="auto" height="auto" max-width="80%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Simplified Transformer Structure (Click on image to zoom in) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/transformer-en-decoders-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/transformer-en-decoders-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/transformer-en-decoders-1400.webp"/> <img src="/assets/img/transformer-analysis/transformer-en-decoders.png" class="img-fluid d-block mx-auto rounded z-depth-1" width="auto" height="auto" max-width="80%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Transformer encoders and decoders (Click on image to zoom in) </div> <p>Transformer model stacks 6 encoders and 6 decoders, ahead by a source embedding and a target embedding networks, end with a softmax classifier. Embedding units can be self-constructed algorithms or pre-trained networks such as <code class="language-plaintext highlighter-rouge">Word2Vec</code>. The encoder and decoder have a very similar structure, only that the decoder has one extra layer of <code class="language-plaintext highlighter-rouge">ADD &amp; NORM</code> and one extra layer of <code class="language-plaintext highlighter-rouge">Multi-head Attention</code>. In addition, it should be noted that a mask operation has been added to the 1st <code class="language-plaintext highlighter-rouge">Multi-head Attention</code> layer of each decoder, which aims to use the <code class="language-plaintext highlighter-rouge">seq2seq</code> structure while ensuring the parallelism of the network. The softmax classifier consists of a <code class="language-plaintext highlighter-rouge">Linear Transformation</code> and a <code class="language-plaintext highlighter-rouge">Softmax</code> layer. The <code class="language-plaintext highlighter-rouge">Linear Transformation</code> maps the word vector to <code class="language-plaintext highlighter-rouge">one-hot</code> vector whose size is equal to the size of the dictionary. Let’s analyzing these 4 kinds of units in detail while deriving Transformer’s forward propagation.</p> <h2 id="forward-propagation">Forward Propagation</h2> <hr/> <h3 id="word-embedding">Word Embedding</h3> <p>First let’s see what is <code class="language-plaintext highlighter-rouge">one-hot</code> encoding. The input of word embedding network is <code class="language-plaintext highlighter-rouge">ont-hot</code> word vector. Say we have 32000 words in a English lexicon ranking in dictionary order. <code class="language-plaintext highlighter-rouge">One hot</code> method transforms each word to a vector of length 32000 with 31999 zero values and 1 one value locating at the word’s index in lexicon. For example, we all know (At least every Chinese student knows) the <strong>abandon</strong> ranks no.1 in all High School English dictionaries of China. That make <strong>abandon</strong>’s <code class="language-plaintext highlighter-rouge">one-hot</code> vector be $[1, 0, 0, …0]$, vector length = 32000. Same as other words.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/one-hot-encoding-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/one-hot-encoding-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/one-hot-encoding-1400.webp"/> <img src="/assets/img/transformer-analysis/one-hot-encoding.jpg" class="img-fluid d-block mx-auto rounded z-depth-1" width="auto" height="auto" max-width="80%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> one hot encoding </div> <p><code class="language-plaintext highlighter-rouge">Word Embedding</code> maps words into intensive vectors. An intensive vector is such a vector of length far smaller than the dictionary size and indicating the relationship of words by their space distrution.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transformer-analysis/word-embedding-space-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transformer-analysis/word-embedding-space-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transformer-analysis/word-embedding-space-1400.webp"/> <img src="/assets/img/transformer-analysis/word-embedding-space.jpeg" class="img-fluid d-block mx-auto rounded z-depth-1" width="auto" height="auto" max-width="80%" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> word embedding space (Chinese characters as an example), come from [csdn blog](https://blog.csdn.net/Alex_81D/article/details/114287498) </div>]]></content><author><name></name></author><category term="deep-learning"/><category term="Transformer"/><summary type="html"><![CDATA[a plain introduction of Transformer and professional derivation of its forward and backward propagation]]></summary></entry></feed>