<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://shawnzou717.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shawnzou717.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-04T10:02:26+08:00</updated><id>https://shawnzou717.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Transformer Analysis</title><link href="https://shawnzou717.github.io/blog/2023/transformer-analysis/" rel="alternate" type="text/html" title="Transformer Analysis"/><published>2023-08-03T11:57:00+08:00</published><updated>2023-08-03T11:57:00+08:00</updated><id>https://shawnzou717.github.io/blog/2023/transformer-analysis</id><content type="html" xml:base="https://shawnzou717.github.io/blog/2023/transformer-analysis/"><![CDATA[<h2 id="table-of-contents">Table of Contents</h2> <ul> <li><a href="#table-of-contents">Table of Contents</a></li> <li><a href="#whats-transformer">What’s Transformer?</a></li> </ul> <p>Among all the deep learning models, Transformer can be the most widely talked over one thanks to the tremendous performance that ChatGPT, a Transformer-based model, has shown in being a satisfying personal assistant and nice chatbot friend. As a powerful mathematical tool, Transformer has helped us in language translation, DNA recognition, medical research and many aspects in other research area. I believe it is safe to say that one day we all may need to apply this model in our project. Thus, a solid understanding of Transformer architecture is neccesary. To this end, this blog focuses on derivation of detailed functions in transformer tranining process.</p> <h2 id="whats-transformer">What’s Transformer?</h2> <p>Tranformer is an advanced neural network model first proposed by Google in the paper <a href="https://arxiv.org/abs/1706.03762v4">Attention is All You Need</a>. Transformer consists of decoders and encoders (also known as <code class="language-plaintext highlighter-rouge">seq2seq</code> structure) like most of the NLP models, but instead of using RNN (Recurrent Neural Networks) and CNN (Convolution Neural Networks), .</p>]]></content><author><name></name></author><category term="deep-learning"/><category term="Transformer"/><summary type="html"><![CDATA[function derivation in forward and backward propagation of transformer]]></summary></entry></feed>